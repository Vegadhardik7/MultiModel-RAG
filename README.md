# MultiModel-RAG 

![](output-image.png)

# MultiModel-RAG Architecture Flow

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   User uploads PDF       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ partition_pdf() (Unstructured)   â”‚
                   â”‚ - Text (NarrativeText, Title)    â”‚
                   â”‚ - Tables                         â”‚
                   â”‚ - Images                         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚                        â”‚
          â–¼                       â–¼                        â–¼
  Text Elements             Table Elements          Image Elements
  (len > 80)                _linearize_table()      _describe_image()
                                                    â”‚
                                                    â””â”€ CLIP (ViT-B/32)
                                                       â†’ Visual label
                                                       â†’ Image context text
          
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                        Combined Text Chunks
                 (Text / "Table: ..." / "Image context: ...")
                                  â”‚
                                  â–¼
                     embed_texts() â†’ SentenceTransformer
                     (all-mpnet-base-v2)
                                  â”‚
                                  â–¼
                        Chroma Vector Store
                (Session-specific PersistentClient)
                                  â”‚
                                  â–¼
                         â”€â”€â”€â”€â”€â”€â”€â”€â”€RAG PHASEâ”€â”€â”€â”€â”€â”€â”€â”€â”€
                                  â”‚
User Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
embed_texts(query)
      â”‚
      â–¼
retrieve() from Chroma (top k=6)
      â”‚
      â–¼
build_prompt()
  - Inject retrieved chunks
  - Inject conversation history
      â”‚
      â–¼
Ollama LLM (generate / generate_stream)
      â”‚
      â–¼
Final Answer
      â”‚
      â–¼
Stored in Session Memory (last 4 turns)

## ğŸ“Œ **High-Level Summary of the Project (from Code & Structure)**

**Project Title:** *Llama 3.1-8B-4.7GB + CLIP + chromadb* ([GitHub][1])

This implies the system uses:

* **Local LLM (likely Llama 3.1 8B)** â€” for generation
* **CLIP model** â€” for vision/text multimodal embeddings
* **ChromaDB** â€” as the vector database for retrieval

### **Explain the complete flow of this MultiModel RAG application.**  

This application is built around a multimodal retrieval pipeline with four main stages: **ingestion, embedding, retrieval, and generation**.  

1. **Ingestion:**  
   The system starts by taking in a PDF document. Using the `partition_pdf()` function from the Unstructured library, the document is broken down into three types of content: text, tables, and images.  
   - Text is cleaned and filtered based on length.  
   - Tables are converted into linearized text so they can be processed like normal text.  
   - Images go through a visual understanding pipeline using the CLIP model (ViT-B/32). CLIP generates descriptive text from the image, making it searchable in the same space as text and tables.  

2. **Embedding:**  
   Once all content is converted into text chunks (plain text, table text, and image descriptions), these chunks are embedded using the SentenceTransformer model `all-mpnet-base-v2`. The embeddings are stored in a **Chroma vector database**, with each session having its own persistent client to ensure isolation.  

3. **Retrieval:**  
   When a user asks a question, the query itself is embedded using the same SentenceTransformer model. The system then performs a similarity search in Chroma to find the most relevant chunks (top-k results).  

4. **Generation:**  
   The retrieved chunks are combined with the userâ€™s recent conversation history to form a structured prompt. This prompt is sent to the Ollama LLM, which generates a response. The output can be streamed or returned as a complete answer, and the conversation history is updated for continuity.  

**In summary:**  
The system takes text, tables, and images from a PDF, converts them into a unified embedding space, stores them in a vector database, retrieves the most relevant information when queried, and finally uses an LLM to generate contextual answers. This makes the application capable of handling multimodal inputs while delivering precise, conversational outputs.  

---

## ğŸ“ **Repository Structure**

The main folders visible in the project are:

* **`app/`** â€” application source code (backend logic) ([GitHub][1])
* **`chroma_sessions/`** â€” likely stores session or vector store states ([GitHub][1])
* **`frontend/`** â€” front-end UI (HTML/JS) for interacting with the app ([GitHub][1])
* **`memory/`** â€” memory storage/caching mechanisms ([GitHub][1])
* **`scripts/`** â€” utility scripts (install/setup helpers) ([GitHub][1])
* **`output-image.png`** â€” image preview showing sample output behavior ([GitHub][1])
* **`requirements.txt`** â€” Python dependencies ([GitHub][2])
* **`structure.py`** â€” script to generate the project skeleton ([GitHub][3])

---

## ğŸ§  **Dependencies (From `requirements.txt`)**

The project uses the following libraries (implying system capabilities): ([GitHub][2])

* **FastAPI** + **uvicorn** â€” backend API server
* **Requests, pillow** â€” HTTP and image handling
* **sentence-transformers** / **torch / transformers** â€” text embedding models
* **chromadb** â€” vector database
* **unstructured** â€” document parsing (esp. for PDFs/text)
* **python-magic-bin** / **huggingface_hub** â€” file metadata + HF model downloads

---

## ğŸ§± **Inferred System Modules (From `structure.py`)**

Although these files arenâ€™t fully visible, the structure builder shows what functional modules *would* exist: ([GitHub][3])

### Ingestion

* `text_ingest.py` â†’ for ingesting text docs
* `image_ingest.py` â†’ for ingesting images

### Embedding

* `text_embedder.py` â†’ text embeddings
* `image_embedder.py` â†’ multimodal (vision) embeddings

### Vector Store

* `chroma_client.py` â†’ connects to ChromaDB

### Retriever

* `retriever.py` â†’ retrieves relevant chunks from ChromaDB

### LLM Interface

* `ollama_client.py` â†’ interface to local LLM (likely Llama)

### RAG Pipeline

* `rag_pipeline.py` â†’ puts together embed â†’ retrieve â†’ generate

### API

* `api.py` â†’ FastAPI routes for frontend/backend interaction

### Auxiliary

* `setup_check.py` â†’ checks environment setup

---

## ğŸ“Š **Putting It All Together â€” *What the System Does***

From the structure and naming alone, we can infer the **data flow**:

1. **Ingestion**

   * User uploads text/image files via the frontend.
   * The system parses ingestion files (`text_ingest`, `image_ingest`).

2. **Embedding**

   * Text sections converted to embeddings using sentence models.
   * Images converted to vector embeddings using CLIP.

3. **Storage**

   * Embeddings saved into **ChromaDB**.

4. **Retrieval**

   * For any user query, the retriever fetches the top relevant vectors from Chroma.

5. **Generation**

   * Retrieved context passed to the LLM (via `ollama_client`) for answer generation.

6. **API / UI**

   * Frontend sends user request to FastAPI backend, which responds with results.

This is a classic **Multimodal Retrieval-Augmented Generation (RAG) pipeline**, combining vision + text modalities. ([Medium][4])
